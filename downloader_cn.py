# -*- coding: utf-8 -*-
import os, sys, time, random, json, subprocess
import pandas as pd
import yfinance as yf
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm

# ========== åƒæ•¸èˆ‡è·¯å¾‘è¨­å®š ==========
MARKET_CODE = "cn-share"
DATA_SUBDIR = "dayK"
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(BASE_DIR, "data", MARKET_CODE, DATA_SUBDIR)
LIST_DIR = os.path.join(BASE_DIR, "data", MARKET_CODE, "lists")
CACHE_LIST_PATH = os.path.join(LIST_DIR, "cn_stock_list_cache.json")

# ğŸ›¡ï¸ ç©©å®šæ€§å„ªå…ˆï¼šä¿æŒ 4 å€‹åŸ·è¡Œç·’ï¼Œé¿å…è§¸ç™¼å°é–
THREADS_CN = 4 
os.makedirs(DATA_DIR, exist_ok=True)
os.makedirs(LIST_DIR, exist_ok=True)

# ğŸ’¡ å®šç¾©æ•¸æ“šéæœŸæ™‚é–“ (3600 ç§’ = 1 å°æ™‚)
DATA_EXPIRY_SECONDS = 3600

def log(msg: str):
    print(f"{pd.Timestamp.now():%H:%M:%S}: {msg}")

def ensure_pkg(pkg: str):
    """è‡ªå‹•æª¢æŸ¥ä¸¦å®‰è£å¿…è¦çš„å¥—ä»¶"""
    try:
        __import__(pkg)
    except ImportError:
        log(f"ğŸ”§ æ­£åœ¨å®‰è£ {pkg}...")
        subprocess.run([sys.executable, "-m", "pip", "install", "-q", pkg])

def get_cn_list():
    """ç²å– A è‚¡æ¸…å–®ï¼šæ•´åˆ EM æ¥å£èˆ‡å¤šé‡ä¿åº•æ©Ÿåˆ¶"""
    ensure_pkg("akshare")
    import akshare as ak
    threshold = 4500  
    
    # 1. æª¢æŸ¥ä»Šæ—¥å¿«å–
    if os.path.exists(CACHE_LIST_PATH):
        try:
            file_mtime = os.path.getmtime(CACHE_LIST_PATH)
            if datetime.fromtimestamp(file_mtime).date() == datetime.now().date():
                with open(CACHE_LIST_PATH, "r", encoding="utf-8") as f:
                    data = json.load(f)
                    if len(data) >= threshold:
                        log(f"ğŸ“¦ è¼‰å…¥ä»Šæ—¥å¿«å– (å…± {len(data)} æª”)")
                        return data
        except Exception:
            pass

    log("ğŸ“¡ å˜—è©¦å¾ Akshare EM æ¥å£ç²å–æ¸…å–®...")
    try:
        df_sh = ak.stock_sh_a_spot_em()
        df_sz = ak.stock_sz_a_spot_em()
        df = pd.concat([df_sh, df_sz], ignore_index=True)
        
        df['code'] = df['ä»£ç '].astype(str).str.zfill(6)
        valid_prefixes = ('000','001','002','003','300','301','600','601','603','605','688')
        df = df[df['code'].str.startswith(valid_prefixes)]
        
        name_col = 'åç§°' if 'åç§°' in df.columns else 'åç¨±'
        res = [f"{row['code']}&{row[name_col]}" for _, row in df.iterrows()]
        
        if len(res) >= threshold:
            with open(CACHE_LIST_PATH, "w", encoding="utf-8") as f:
                json.dump(res, f, ensure_ascii=False)
            log(f"âœ… æˆåŠŸç²å– {len(res)} æª”æ¨™çš„")
            return res
    except Exception as e:
        log(f"âš ï¸ EM æ¥å£å¤±æ•—: {e}")

    if os.path.exists(CACHE_LIST_PATH):
        with open(CACHE_LIST_PATH, "r", encoding="utf-8") as f:
            return json.load(f)

    return ["600519&è²´å·èŒ…å°", "000001&å¹³å®‰éŠ€è¡Œ", "300750&å¯§å¾·æ™‚ä»£"]

def download_one(item):
    """å–®æª”ä¸‹è¼‰é‚è¼¯ï¼šå…·å‚™æ™‚æ•ˆæª¢æŸ¥èˆ‡å¼·åŒ–é˜²å°é–"""
    try:
        code, name = item.split('&', 1)
        symbol = f"{code}.SS" if code.startswith('6') else f"{code}.SZ"
        out_path = os.path.join(DATA_DIR, f"{code}_{name}.csv")

        # ğŸ’¡ æ™ºæ…§æ™‚æ•ˆæª¢æŸ¥
        if os.path.exists(out_path):
            file_age = time.time() - os.path.getmtime(out_path)
            # è‹¥æª”æ¡ˆå­˜åœ¨ä¸”å°æ–¼ 1 å°æ™‚å‰‡è·³é
            if file_age < DATA_EXPIRY_SECONDS and os.path.getsize(out_path) > 1000:
                return {"status": "exists", "code": code}

        max_retries = 3
        for attempt in range(max_retries):
            try:
                # ğŸ›¡ï¸ éš¨æ©Ÿå»¶é²ä¿è­·
                time.sleep(random.uniform(0.7, 1.5)) 
                
                tk = yf.Ticker(symbol)
                hist = tk.history(period="2y", timeout=25)
                
                if hist is not None and not hist.empty:
                    hist.reset_index(inplace=True)
                    hist.columns = [c.lower() for c in hist.columns]
                    if 'date' in hist.columns:
                        hist['date'] = pd.to_datetime(hist['date'], utc=True).dt.tz_localize(None)
                    
                    hist.to_csv(out_path, index=False, encoding='utf-8-sig')
                    return {"status": "success", "code": code}
                
                if attempt == max_retries - 1:
                    return {"status": "empty", "code": code}
                    
            except Exception:
                if attempt == max_retries - 1:
                    return {"status": "error", "code": code}
                time.sleep(random.randint(5, 12)) 
    except Exception:
        return {"status": "error", "code": item.split('&')[0]}
            
    return {"status": "error", "code": code}

def main():
    start_time = time.time()
    log("ğŸ‡¨ğŸ‡³ ä¸­åœ‹ A è‚¡åŒæ­¥å™¨ (æ™‚æ•ˆæª¢æŸ¥æ¨¡å¼)")
    
    items = get_cn_list()
    log(f"ğŸš€ ç›®æ¨™ç¸½æ•¸: {len(items)} æª”")
    
    stats = {"success": 0, "exists": 0, "empty": 0, "error": 0}
    
    with ThreadPoolExecutor(max_workers=THREADS_CN) as executor:
        futures = {executor.submit(download_one, it): it for it in items}
        pbar = tqdm(total=len(items), desc="ä¸‹è¼‰é€²åº¦")
        
        for f in as_completed(futures):
            res = f.result()
            stats[res.get("status", "error")] += 1
            pbar.update(1)
        pbar.close()

    total_expected = len(items)
    effective_success = stats['success'] + stats['exists']
    fail_count = stats['error'] + stats['empty']

    download_stats = {
        "total": total_expected,
        "success": effective_success,
        "fail": fail_count
    }

    duration = (time.time() - start_time) / 60
    log(f"ğŸ“Š åŸ·è¡Œå ±å‘Š: æˆåŠŸ(å«æ•ˆæœŸå…§)={effective_success}, å¤±æ•—={fail_count}, è€—æ™‚={duration:.1f}åˆ†é˜")
    
    return download_stats

if __name__ == "__main__":
    main()
